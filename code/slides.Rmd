---
title: "A survival guide to large-scale data analysis in R"
author: Peter Carbonetto
output:
  beamer_presentation:
    template: beamer.tex
    keep_tex: false
---

```{r knitr-options, echo=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,fig.align = "center",
                      results = "hide", fig.show = "hide", message = FALSE,
		      warning = FALSE)
```

Outline of tutorial
===================

1. Item 1.

2. Item 2.

3. Item 3.

4. Item 4.

It's your choice
================

Your may choose to...

+ Work through the examples on the RCC cluster.

+ Work through the examples on your laptop.

+ Pair up with your neighbour.

+ Follow what I do on the projector.

*However*, note:

1. A few of the examples will only run on the RCC cluster.

2. The examples may not produce the exact same result on your laptop.

Software requirements
====================

1. R (ideally,  version 3.4.0 or greater)

2. python 3.x

3. SLURM (this is the job scheduler on the RCC cluster)

Initial setup (part 1)
======================

+ WiFi

+ Power outlets

+ YubiKeys

+ Pace, questions (e.g., keyboard shortcuts).

Initial setup (part 2)
======================

Download the workshop packet to your laptop.

+ URL: https://github.com/pcarbo/R-survival-large-scale

+ Open **slides.pdf** from the **docs** folder. This is useful for
viewing and copying the code from the slides.

Initial setup (part 3)
======================

If you are using the RCC cluster, set up your cluster computing
environment:

+ Connect to midway2.

    - See https://rcc.uchicago.edu/docs/connecting

+ Request a midway2 ("Broadwell") compute node with 8 CPUs and 15 GB
of memory:

    ```
    screen -S r-tutorial  # Optional.
    sinteractive --partition=broadwl --mem=15G \
      --cpus-per-task=8 --time=3:00:00
    ```

+ Make note of the compute node you have connected to:

    ```
    echo $HOSTNAME
    ```

Initial setup (part 4)
======================

If you are using the RCC cluster, run these commands to download the
workshop packet to your home directory (no spaces in URL):

```{bash, eval=FALSE}
cd $HOME
git clone https://github.com/pcarbo/ 
  R-survival-large-scale.git
```

Initial setup (part 5)
======================

Next, we set up **htop** for monitoring processes.

Open up a separate Terminal window and connect to midway2.

Connect to the same compute node:

```{bash, eval=FALSE}
ssh $HOSTNAME
```

Copy my htop configuration:

```{bash, eval=FALSE}
mkdir -p ~/.config/htop
cp extras/htoprc ~/.config/htop/
```

Start htop:

```{bash, eval=FALSE}
htop --user=<username>
```

Some useful htop keyboard shortcuts:

+ q to quit
+ M to sort processes by memory
+ p to toggle program paths
+ H to toggle threads (for multithreaded computing)

Initial setup (part 6)
======================

Move to the **code** folder in the git repository, and start up an
interactive R environment:

```{bash, eval=FALSE}
cd code
module load R
R --no-save
```

Check which version of R you are running (hopefully version 3.4.0 or
greater):

```{r check-r-version}
version$version.string
```

Also, before continuing, check your working directory:

```{r check-wd}
getwd()  # Should be .../code
```


What's in the workshop packet
=============================

```
R-survival-large-scale
  /code    # Source code used in demos.
  /data    # "Raw" and processed data.
  /docs    # Slides and other materials.
  /extras  # Additional useful files.
  /output  # All results are stored here.
```

Investigation: How to measure memory in R, and use it effectively
=================================================================

We have two tasks:

1. Compute the SNP minor allele frequencies (MAFs) from the a large
genotype data matrix.

2. Center and scale the columns of the genotype matrix.

We will use R for both tasks.

+ We will monitor memory allocation in R, and find ways to reduce it.

+ To monitor memory allocation, we will use:

    1. **htop**

    2. **monitor_memory.py**, a Python script.

Vignette: How to measure memory, and use it effectively
=======================================================

Outline of the vignette:

1. Simulate a genotype data matrix, and save the data.

2. Monitor memory needed to compute MAFs.

3. Monitor memory needed to scale genotypes.

4. Attempt to reduce memory used in scaling genotypes.

Generate genotype data
======================

We generate an $n \times p$ genotype matrix with $n = 1000$ samples,
$p = 300,\!000$ SNPs and 1% missing genotypes.

```{r, sim-geno}
source("sim.geno.R")
set.seed(1)
geno <- sim.geno(n = 1000,p = 3e5,na.rate = 0.01)
dim(geno)
range(geno,na.rm = TRUE)
mean(is.na(geno))
save(list = "geno",file = "../data/geno.RData")
```

Compute and summarize MAFs
==========================

We will run this code non-interactively to compute the MAFs:

```{r summarize-maf}
# Compute MAFs.
maf <- colMeans(geno,na.rm = TRUE)/2
maf <- pmin(maf,1 - maf)
# Summarize distribution of MAFs.
print(quantile(maf,seq(0,1,0.1)),digits = 2)
```

+ Quit R.

+ Run these commands in the shell:

    ```
    pwd # Should be .../code
    Rscript summarize.maf.R
    ```

Monitoring memory allocation
============================

While the R script is running, use **htop** in a separate Terminal
session to profile memory allocation (look at the **RES** column).

+ If necessary, re-run the code.

Re-run the R script again, this time using the Python program to
measure memory usage every 0.01 s:

```{bash, eval=FALSE}
export MEM_CHECK_INTERVAL=0.01
./monitor_memory.py Rscript summarize.maf.R
```

How well does the **htop** estimate agree with **max rss_memory** from
the Python script?

Center and scale the genotype matrix
====================================

Next, we will run this code non-interactively to center and scale the
genotype matrix:

```{r scale-geno}
source("scale.R")
cat("Loading genotype data.\n")
load("../data/geno.RData")
cat("Centering and scaling genotype matrix.\n")
geno <- scale(geno)
```

Using the same methods as before, monitor memory usage.

```{bash, eval=FALSE}
./monitor_memory.py Rscript scale.geno.R
```

Does the scaling operation require more memory than computing MAFs?

+ If so, how much more?

+ How do you explain this result?

A better implementation of "scale"
==================================

Motivated by out observations in the previous slide, let's try to
reduce the memory usage by writing our own **scale** function.

```{r scale-geno-better}
scale_better <- function (X) {
  for (i in 1:ncol(X)) {
    X[,i] <- X[,i] - mean(X[,i],na.rm = TRUE)
    X[,i] <- X[,i] / sd(X[,i],na.rm = TRUE)
  }
  return(X)
}
```

Now run the script that uses our custom scaling function:

```{bash, eval=FALSE}
./monitor_memory.py Rscript scale.geno.better.R
```

Does **scale_better** require less memory than **scale**? Why? Is
there more room to reduce memory usage?

Matrix scaling: a second attempt 
================================

This next script attempts to reduce memory usage even further by
avoiding any function calls that modify `geno` within a local
environment.

```{bash, eval=FALSE}
./monitor_memory.py Rscript \
  scale.geno.even.better.R
```

This is the code:

```{r scale-geno-even-better}
load("../data/geno.RData")
for (i in 1:ncol(geno)) {
  x        <- geno[,i]
  x        <- x - mean(x,na.rm = TRUE)
  x        <- x / sd(x,na.rm = TRUE)
  geno[,i] <- x
}
```

Is this an improvement over our previous attempt? Is there still room
for improvement?

Matrix scaling: one last attempt using Rcpp
===========================================

The root issue is that R does not allow for "copying in place".

+ We can circumvent this by implementing a C++ function that modifies
the input matrix directly.

+ See files **scale.cpp** and **scale.geno.rcpp.R** for how this is
implemented using the **Rcpp** package.

Now try monitoring memory usage in the script that uses the Rcpp
implementation:

```{bash, eval=FALSE}
./monitor_memory.py Rscript scale.geno.rcpp.R
```

Does this implementation have a smaller memory footprint than previous
attempts? Is there still room for improvement?

Memory allocation in R: take-home points
========================================

+ Important to measure memory in a fresh R session.

+ R duplicates objects aggressively.

+ Rcpp can circumvent many of these issues but requires much more work!

More advanced tools:
+ http://adv-r.had.co.nz/memory.html
+ profmem R package
