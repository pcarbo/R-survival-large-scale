---
title: "A survival guide to large-scale data analysis in R"
author: Peter Carbonetto
output:
  beamer_presentation:
    template: beamer.tex
    keep_tex: false
---

```{r knitr-options, echo=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,fig.align = "center",
                      results = "hide", fig.show = "hide", message = FALSE,
		      warning = FALSE)
```

Outline of tutorial
===================

1. Item 1.

2. Item 2.

3. Item 3.

4. Item 4.

It's your choice
================

Your may choose to...

+ Work through the examples on the RCC cluster.

+ Work through the examples on your laptop.

+ Pair up with your neighbour.

+ Follow what I do on the projector.

*However*, note:

1. A few of the examples will only run on the RCC cluster.

2. The examples may not produce the exact same result on your laptop.

Software requirements
====================

1. R (ideally,  version 3.4.0 or greater)

2. python 3.x

3. SLURM (this is the job scheduler on the RCC cluster)

Initial setup (part 1)
======================

+ WiFi

+ Power outlets

+ YubiKeys

+ Pace, questions (e.g., keyboard shortcuts).

Initial setup (part 2)
======================

Download the workshop packet to your laptop.

+ URL: https://github.com/pcarbo/R-survival-large-scale

+ Open **slides.pdf** from the **docs** folder. This is useful for
viewing and copying the code from the slides.

Initial setup (part 3)
======================

If you are using the RCC cluster, set up your cluster computing
environment:

+ Connect to midway2.

    - See https://rcc.uchicago.edu/docs/connecting

+ Request a midway2 ("Broadwell") compute node with 8 CPUs and 15 GB
of memory:

    ```
    screen -S r-tutorial  # Optional.
    sinteractive --partition=broadwl --mem=15G \
      --cpus-per-task=8 --time=3:00:00
    ```

+ Make note of the compute node you have connected to:

    ```
    echo $HOSTNAME
    ```

Initial setup (part 4)
======================

If you are using the RCC cluster, run these commands to download the
workshop packet to your home directory (no spaces in URL):

```
cd $HOME
git clone https://github.com/pcarbo/ 
  R-survival-large-scale.git
```

Initial setup (part 5)
======================

Next, we set up **htop** for monitoring processes.

Open up a separate Terminal window and connect to midway2.

Connect to the same compute node:

```
ssh $HOSTNAME
```

Copy my htop configuration:

```
mkdir -p ~/.config/htop
cp extras/htoprc ~/.config/htop/
```

Start htop:

```
htop --user=<username>
```

Some useful htop keyboard shortcuts:

+ q to quit
+ M to sort processes by memory
+ p to toggle program paths
+ H to toggle threads (for multithreaded computing)

Initial setup (part 6)
======================

Move to the **code** folder in the git repository, and start up an
interactive R environment:

```{bash, eval=FALSE}
cd code
module load R
R --no-save
```

Check which version of R you are running (hopefully version 3.4.0 or
greater):

```{r check-r-version}
version$version.string
```

Also, before continuing, check your working directory:

```{r check-wd}
getwd()  # Should be .../code
```


What's in the workshop packet
=============================

```
R-survival-large-scale
  /code    # Source code used in demos.
  /data    # "Raw" and processed data.
  /docs    # Slides and other materials.
  /extras  # Additional useful files.
  /output  # All results are stored here.
```

Investigation: How to measure memory in R, and use it effectively
=================================================================

We have two tasks:

1. Compute the SNP minor allele frequencies (MAFs) from the a large
genotype data matrix.

2. Center and scale the columns of the genotype matrix.

We will use R for both tasks.

+ We will monitor memory usage in R, and find ways to reduce memory
usage.

+ To monitor memory usage, we will use:

    1. **htop**

    2. **monitor_memory.py**, a Python script.

Vignette: How to measure memory, and use it effectively
=======================================================

Outline of the vignette:

1. Simulate a genotype data matrix, and save the data.

2. Monitor memory needed to compute MAFs.

3. Monitor memory needed to scale genotypes.

4. Attempt to reduce memory used in scaling matrix columns.

Generate genotype data
======================

We generate an $n \times p$ genotype matrix with $n = 1000$ samples,
$p = 300,000 SNPs) and 1% missing genotypes.

```{r, sim-geno}
source("sim.geno.R")
geno <- sim.geno(n = 100,p = 3e5,na.rate = 0.01)
dim(geno)
range(geno,na.rm = TRUE)
mean(is.na(geno))
save(list = "geno",file = "../data/geno.RData")
```

Compute and summarize minor allele frequencies
==============================================

+ Quit R.
+ Compute and summarize the minor allele frequencies:
    export MEM_CHECK_INTERVAL=0.01
    ./monitor_memory.py Rscript summarize.maf.R
+ Center and scale the genotype matrix using "scale" function:
    ./monitor_memory.py Rscript scale.geno.R
+ Center and scale the genotype matrix using my "scale_better" function:
    ./monitor_memory.py Rscript scale.geno.better.R
+ Center and scale the genotype matrix without calling a function:
    ./monitor_memory.py Rscript scale.geno.even.better.R
+ Center and scale the genotype matrix in place using Rcpp:
    ./monitor_memory.py Rscript scale.geno.best.R

More advanced:
+ http://adv-r.had.co.nz/memory.html
+ profmem R package